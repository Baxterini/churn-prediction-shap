{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423ea18f-e86d-42ae-b376-a24f97fcb11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost (test) ===\n",
      "ROC-AUC:  0.856109\n",
      "F1:       0.602941 (threshold=0.5)\n",
      "\n",
      "Confusion matrix:\n",
      "[[1335  258]\n",
      " [ 120  287]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9175    0.8380    0.8760      1593\n",
      "           1     0.5266    0.7052    0.6029       407\n",
      "\n",
      "    accuracy                         0.8110      2000\n",
      "   macro avg     0.7221    0.7716    0.7395      2000\n",
      "weighted avg     0.8380    0.8110    0.8204      2000\n",
      "\n",
      "\n",
      "✅ Zapisano model: C:\\Users\\baxiu\\Desktop\\od_zera_do_ai\\bank-churn-app\\models\\churn_xgb_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_xgb.py - Bank Churn model: XGBoost (B2)\n",
    "\n",
    "Cel:\n",
    "- Zbudować mocny model boostingowy (często top w tabular data)\n",
    "- Ten sam preprocessing (OneHot + imputacja)\n",
    "- Metryki (ROC-AUC, F1, confusion matrix)\n",
    "- Zapis pipeline do pliku (joblib) -> do Streamlit / threshold tuning\n",
    "\n",
    "Uwaga:\n",
    "- XGBoost to \"power tool\" -> daje często lepszy ROC-AUC/F1 niż RF\n",
    "- scale_pos_weight pomaga na niezbalansowanie klas (ok. 80/20)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"data/churn.csv\")\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"churn_xgb_pipeline.joblib\"\n",
    "\n",
    "\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Nie znaleziono pliku: {path.resolve()}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if \"Exited\" not in df.columns:\n",
    "        raise ValueError(\"Brakuje kolumny targetu 'Exited'.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_features(df: pd.DataFrame):\n",
    "    drop_cols = [\"RowNumber\", \"CustomerId\", \"Surname\"]\n",
    "    X = df.drop(columns=drop_cols + [\"Exited\"], errors=\"ignore\")\n",
    "    y = df[\"Exited\"].astype(int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate(pipe: Pipeline, X_test: pd.DataFrame, y_test: pd.Series, threshold: float = 0.5) -> None:\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    roc = roc_auc_score(y_test, y_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n=== XGBoost (test) ===\")\n",
    "    print(f\"ROC-AUC:  {roc:.6f}\")\n",
    "    print(f\"F1:       {f1:.6f} (threshold={threshold})\")\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = load_data(DATA_PATH)\n",
    "    X, y = prepare_features(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    # scale_pos_weight = (#neg / #pos) -> pomaga na niezbalansowanie klas\n",
    "    neg = int((y_train == 0).sum())\n",
    "    pos = int((y_train == 1).sum())\n",
    "    scale_pos_weight = neg / max(pos, 1)\n",
    "\n",
    "    preprocessor = build_preprocessor(X_train)\n",
    "\n",
    "    # Sensowny, \"bezpieczny\" start pod tabular data\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",   # szybciej na CPU\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", xgb)])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # baseline threshold 0.5\n",
    "    evaluate(pipe, X_test, y_test, threshold=0.5)\n",
    "\n",
    "    joblib.dump(pipe, MODEL_PATH)\n",
    "    print(f\"\\n✅ Zapisano model: {MODEL_PATH.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
